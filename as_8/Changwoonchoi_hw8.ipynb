{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWo4xJnlTUiy"
   },
   "source": [
    "M2780.002400 Machine Listening (Fall 2020)\n",
    "\n",
    "Instructor: Kyogu Lee (kglee@snu.ac.kr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jB94TKZiTbPX"
   },
   "source": [
    "# Assignment 8: Advanced Neural Networks\n",
    "\n",
    "(40 points)\n",
    "\n",
    "Due Date : This assignment is due by 12:59PM, November 24 (Tuesday)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Assignment\n",
    "\n",
    "This assignment is composed of only one part: Lab assignment, you will need to write Notebook scripts and/or functions as required and submit them electronically (via **ETL**) by the end of the due date. Before the submission, please make sure that the file name is in form of **아무개_hw8.ipynb**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import glob\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, pooling\n",
    "from keras.optimizers import Adam\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsjE1gXxXYO6"
   },
   "source": [
    "# Lab (40pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvvsRSFJXiH2"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "1.   Speech MNIST using CNN (20 pts)\n",
    "\n",
    "  a) Record your speech saying zero, one, two, ..., nine. (You don't need to submit these recordings) Refer to the lab8 materials to create and learn a neural network that classifies spoken digit dataset. Use the trained model to classify your recordings and report the results. \n",
    "\n",
    "  b) Improve the network performance using the techniques introduced in lab8. You can use more or different types of layers, regularization. Data augmentation methods would be also helpful. Compare the results with a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "a7x19YyHXhRd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with my recording: 0.699999988079071\n"
     ]
    }
   ],
   "source": [
    "# a)\n",
    "\n",
    "model = keras.models.load_model(\"mnist_trained\")\n",
    "\n",
    "Y_my_recording = np.eye(10)  # right answer of my recording\n",
    "\n",
    "# preprocess recorded data and stack to make X_my_recording\n",
    "X = []\n",
    "for i in range(10):\n",
    "    wave = np.load('./my_recording/my_{}.npy'.format(i))\n",
    "    mel = librosa.feature.melspectrogram(wave, sr=8000, n_mels=80)\n",
    "    X.append(mel)\n",
    "data_length = []\n",
    "for item in X:\n",
    "    data_length.append(np.shape(item)[1])\n",
    "def pad(mel, max_length):\n",
    "    if np.shape(mel)[1] > max_length:\n",
    "        return mel[:, :max_length]\n",
    "    else:\n",
    "        return np.concatenate((mel, np.zeros((80, max_length-np.shape(mel)[1]))), axis=1)\n",
    "max_length = 36\n",
    "X_pad = []\n",
    "for item in X:\n",
    "    X_pad.append(pad(item, max_length))\n",
    "X_pad = np.asarray(X_pad)\n",
    "X_pad = np.expand_dims(X_pad, axis=3)\n",
    "X_my_recording = X_pad\n",
    "\n",
    "\n",
    "score = model.evaluate(X_my_recording, Y_my_recording, verbose=0)\n",
    "print(\"Accuracy with my recording:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) \n",
    "\n",
    "# prepare train data with data augmentation\n",
    "\n",
    "import glob\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "speech_mnist_data = []\n",
    "speech_mnist_target = []\n",
    "\n",
    "speech_mnist_aug_data = []\n",
    "speech_mnist_aug_target = []\n",
    "\n",
    "speech_mnist = glob.glob('./lab8/speech_mnist/*.npy')\n",
    "speech_mnist_spec_aug = glob.glob('./lab8/speech_mnist/*.npy')\n",
    "for item in speech_mnist:\n",
    "  target, speaker, index = item.split('/')[-1].split('.npy')[0].split('_')\n",
    "  wavs = np.load(item)\n",
    "  mel = librosa.feature.melspectrogram(wavs, sr=8000, n_mels=80)\n",
    "  speech_mnist_data.append(mel)\n",
    "  speech_mnist_target.append(np.eye(10)[(int)(target)])\n",
    "\n",
    "for item in speech_mnist_spec_aug:\n",
    "    target, speaker, index = item.split('/')[-1].split('.npy')[0].split('_')\n",
    "    wavs = np.load(item)\n",
    "    mel = librosa.feature.melspectrogram(wavs, sr=8000, n_mels = 80)\n",
    "    f_frame = np.random.randint(0, 20)\n",
    "    t_frame = np.random.randint(0, 3)\n",
    "    f_loc = np.random.randint(0, mel.shape[0] - f_frame)\n",
    "    t_loc = np.random.randint(0, mel.shape[1] - t_frame)\n",
    "    for i in range(0, f_frame):\n",
    "        mel[f_loc + i, :] = 0\n",
    "    for i in range(0, t_frame):\n",
    "        mel[:, t_loc + t_frame] = 0\n",
    "    speech_mnist_aug_data.append(mel)\n",
    "    speech_mnist_aug_target.append(np.eye(10)[(int)(target)])\n",
    "\n",
    "data_length = []\n",
    "for item in speech_mnist_data:\n",
    "  data_length.append(np.shape(item)[1])\n",
    "\n",
    "def pad(mel, max_length):\n",
    "  if np.shape(mel)[1] > max_length:\n",
    "    return mel[:,:max_length]\n",
    "  else:\n",
    "    return np.concatenate((mel, np.zeros((80, max_length-np.shape(mel)[1]))), axis=1)\n",
    "\n",
    "max_length = np.max(data_length)\n",
    "speech_mnist_data_pad = []\n",
    "speech_mnist_aug_data_pad = []\n",
    "for item in speech_mnist_data:\n",
    "  speech_mnist_data_pad.append(pad(item, max_length))\n",
    "for item in speech_mnist_aug_data:\n",
    "    speech_mnist_aug_data_pad.append(pad(item, max_length))\n",
    "\n",
    "speech_mnist_data_pad = np.asarray(speech_mnist_data_pad)\n",
    "speech_mnist_data_pad = np.expand_dims(speech_mnist_data_pad, axis=3)\n",
    "speech_mnist_aug_data_pad = np.asarray(speech_mnist_aug_data_pad)\n",
    "speech_mnist_aug_data_pad = np.expand_dims(speech_mnist_aug_data_pad, axis=3)\n",
    "speech_mnist_data_all_pad = np.concatenate((speech_mnist_data_pad, speech_mnist_aug_data_pad), axis=0)\n",
    "speech_mnist_target_all = np.concatenate((speech_mnist_target, speech_mnist_aug_target), axis=0)\n",
    "speech_mnist_target = np.asarray(speech_mnist_target)\n",
    "\n",
    "X_train, _, Y_train, _ = train_test_split(speech_mnist_data_all_pad, speech_mnist_target_all, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new model(decreased the dropout rate)\n",
    "\n",
    "new_model = Sequential()\n",
    "new_model.add(Conv2D(32,(3,3),activation='relu', input_shape=(80,36,1)))\n",
    "new_model.add(Conv2D(32,(3,3),activation='relu'))\n",
    "new_model.add(pooling.MaxPooling2D(pool_size=(2,2)))\n",
    "new_model.add(Dropout(0.2))\n",
    "new_model.add(Conv2D(32,(3,3),activation='relu'))\n",
    "new_model.add(Conv2D(32,(3,3),activation='relu'))\n",
    "new_model.add(pooling.MaxPooling2D(pool_size=(2,2)))\n",
    "new_model.add(Dropout(0.2))\n",
    "new_model.add(Conv2D(32,(3,3),activation='relu'))\n",
    "new_model.add(Conv2D(32,(3,3),activation='relu'))\n",
    "new_model.add(pooling.MaxPooling2D(pool_size=(2,2)))\n",
    "new_model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "new_model.add(Flatten())\n",
    "new_model.add(Dense(128, activation='relu'))\n",
    "new_model.add(Dropout(0.5))\n",
    "new_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "opt = Adam(lr=0.001)\n",
    "new_model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3196, 80, 36, 1)\n",
      "(3196, 10)\n",
      "Epoch 1/50\n",
      "45/45 [==============================] - ETA: 0s - loss: 2.2481 - accuracy: 0.1380WARNING:tensorflow:8 out of the last 24 calls to <function Model.make_test_function.<locals>.test_function at 0x7fceda784c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 2.2481 - accuracy: 0.1380 - val_loss: 2.0885 - val_accuracy: 0.2500\n",
      "Epoch 2/50\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 2.0933 - accuracy: 0.2330 - val_loss: 1.8104 - val_accuracy: 0.4094\n",
      "Epoch 3/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 1.9932 - accuracy: 0.2987 - val_loss: 1.7061 - val_accuracy: 0.4344\n",
      "Epoch 4/50\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 1.8265 - accuracy: 0.3428 - val_loss: 1.4124 - val_accuracy: 0.4844\n",
      "Epoch 5/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 1.5602 - accuracy: 0.4350 - val_loss: 1.1692 - val_accuracy: 0.6031\n",
      "Epoch 6/50\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 1.3510 - accuracy: 0.5170 - val_loss: 1.0398 - val_accuracy: 0.6094\n",
      "Epoch 7/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 1.2948 - accuracy: 0.5348 - val_loss: 0.9404 - val_accuracy: 0.6219\n",
      "Epoch 8/50\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 1.1663 - accuracy: 0.5751 - val_loss: 0.8738 - val_accuracy: 0.6625\n",
      "Epoch 9/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 1.1116 - accuracy: 0.6095 - val_loss: 0.9149 - val_accuracy: 0.6531\n",
      "Epoch 10/50\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 1.0463 - accuracy: 0.6147 - val_loss: 0.7676 - val_accuracy: 0.6781\n",
      "Epoch 11/50\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 0.9721 - accuracy: 0.6481 - val_loss: 0.7044 - val_accuracy: 0.7031\n",
      "Epoch 12/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.9021 - accuracy: 0.6686 - val_loss: 0.6673 - val_accuracy: 0.7469\n",
      "Epoch 13/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.8527 - accuracy: 0.6836 - val_loss: 0.6018 - val_accuracy: 0.7625\n",
      "Epoch 14/50\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 0.8360 - accuracy: 0.6999 - val_loss: 0.6452 - val_accuracy: 0.7469\n",
      "Epoch 15/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.7989 - accuracy: 0.7076 - val_loss: 0.5763 - val_accuracy: 0.7750\n",
      "Epoch 16/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.7430 - accuracy: 0.7333 - val_loss: 0.5418 - val_accuracy: 0.7906\n",
      "Epoch 17/50\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 0.7221 - accuracy: 0.7403 - val_loss: 0.5663 - val_accuracy: 0.7937\n",
      "Epoch 18/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.6988 - accuracy: 0.7524 - val_loss: 0.4808 - val_accuracy: 0.8375\n",
      "Epoch 19/50\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 0.6453 - accuracy: 0.7604 - val_loss: 0.4774 - val_accuracy: 0.8156\n",
      "Epoch 20/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.6081 - accuracy: 0.7782 - val_loss: 0.4472 - val_accuracy: 0.8250\n",
      "Epoch 21/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.5941 - accuracy: 0.7837 - val_loss: 0.4304 - val_accuracy: 0.8406\n",
      "Epoch 22/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.5967 - accuracy: 0.7876 - val_loss: 0.4169 - val_accuracy: 0.8469\n",
      "Epoch 23/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.5868 - accuracy: 0.7973 - val_loss: 0.4052 - val_accuracy: 0.8469\n",
      "Epoch 24/50\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 0.5875 - accuracy: 0.7949 - val_loss: 0.3909 - val_accuracy: 0.8656\n",
      "Epoch 25/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.5496 - accuracy: 0.8001 - val_loss: 0.4182 - val_accuracy: 0.8500\n",
      "Epoch 26/50\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 0.5766 - accuracy: 0.8001 - val_loss: 0.4646 - val_accuracy: 0.8406\n",
      "Epoch 27/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.5645 - accuracy: 0.8077 - val_loss: 0.3542 - val_accuracy: 0.8844\n",
      "Epoch 28/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.5177 - accuracy: 0.8241 - val_loss: 0.3458 - val_accuracy: 0.8781\n",
      "Epoch 29/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.4813 - accuracy: 0.8289 - val_loss: 0.3214 - val_accuracy: 0.8969\n",
      "Epoch 30/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.4713 - accuracy: 0.8359 - val_loss: 0.3076 - val_accuracy: 0.8906\n",
      "Epoch 31/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.4457 - accuracy: 0.8404 - val_loss: 0.3379 - val_accuracy: 0.8781\n",
      "Epoch 32/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.4496 - accuracy: 0.8387 - val_loss: 0.3035 - val_accuracy: 0.9000\n",
      "Epoch 33/50\n",
      "45/45 [==============================] - 4s 99ms/step - loss: 0.4250 - accuracy: 0.8380 - val_loss: 0.3043 - val_accuracy: 0.8844\n",
      "Epoch 34/50\n",
      "45/45 [==============================] - 4s 99ms/step - loss: 0.4176 - accuracy: 0.8456 - val_loss: 0.2739 - val_accuracy: 0.9031\n",
      "Epoch 35/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.4033 - accuracy: 0.8585 - val_loss: 0.3039 - val_accuracy: 0.9094\n",
      "Epoch 36/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.4200 - accuracy: 0.8515 - val_loss: 0.3214 - val_accuracy: 0.8906\n",
      "Epoch 37/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.4339 - accuracy: 0.8387 - val_loss: 0.2720 - val_accuracy: 0.8906\n",
      "Epoch 38/50\n",
      "45/45 [==============================] - 4s 100ms/step - loss: 0.4801 - accuracy: 0.8366 - val_loss: 0.3938 - val_accuracy: 0.8687\n",
      "Epoch 39/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.4115 - accuracy: 0.8574 - val_loss: 0.2763 - val_accuracy: 0.9031\n",
      "Epoch 40/50\n",
      "45/45 [==============================] - 5s 101ms/step - loss: 0.3924 - accuracy: 0.8644 - val_loss: 0.2317 - val_accuracy: 0.9125\n",
      "Epoch 41/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.3614 - accuracy: 0.8762 - val_loss: 0.2455 - val_accuracy: 0.9219\n",
      "Epoch 42/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.3518 - accuracy: 0.8769 - val_loss: 0.2431 - val_accuracy: 0.9062\n",
      "Epoch 43/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.3485 - accuracy: 0.8783 - val_loss: 0.2254 - val_accuracy: 0.9219\n",
      "Epoch 44/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.3372 - accuracy: 0.8853 - val_loss: 0.2216 - val_accuracy: 0.9219\n",
      "Epoch 45/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.3287 - accuracy: 0.8846 - val_loss: 0.2117 - val_accuracy: 0.9250\n",
      "Epoch 46/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.3321 - accuracy: 0.8856 - val_loss: 0.2149 - val_accuracy: 0.9250\n",
      "Epoch 47/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.3555 - accuracy: 0.8842 - val_loss: 0.2030 - val_accuracy: 0.9344\n",
      "Epoch 48/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.3148 - accuracy: 0.8870 - val_loss: 0.2031 - val_accuracy: 0.9438\n",
      "Epoch 49/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.3163 - accuracy: 0.8849 - val_loss: 0.1868 - val_accuracy: 0.9438\n",
      "Epoch 50/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.3065 - accuracy: 0.8877 - val_loss: 0.1908 - val_accuracy: 0.9375\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "hist = new_model.fit(X_train, Y_train, batch_size=64, epochs=50, validation_split = 0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with my recording: 1.0\n"
     ]
    }
   ],
   "source": [
    "score = new_model.evaluate(X_my_recording, Y_my_recording, verbose=0)\n",
    "print(\"Accuracy with my recording:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilgkgKdxdRIB"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "2.   Music Composer using LSTM (20 pts)\n",
    "\n",
    "\n",
    "Refer to the lab8 materials to create and learn a neural network that composes music autoregressively.  Change the input feature and model structure to use 50 notes to generate the next note. Use the trained model to generate 500 notes of music starting from the first 50 notes of DOS.mid on the dataset. Save and submit the generated music as midi file. Your submission should include the trained model weights as hdf5 file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import numpy\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization as BatchNorm\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = './lab8/'\n",
    "\n",
    "def get_notes():\n",
    "    notes = []\n",
    "\n",
    "    for file in glob.glob(location + \"midi_songs/*.mid\")[:5]:\n",
    "        midi = converter.parse(file)\n",
    "        notes_to_parse = None\n",
    "        try: # file has instrument parts\n",
    "            s2 = instrument.partitionByInstrument(midi)\n",
    "            notes_to_parse = s2.parts[0].recurse() \n",
    "        except: # file has notes in a flat structure\n",
    "            notes_to_parse = midi.flat.notes\n",
    "\n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    with open(location + 'data/notes', 'wb') as filepath:\n",
    "        pickle.dump(notes, filepath)\n",
    "\n",
    "    return notes\n",
    "\n",
    "\n",
    "notes = get_notes()\n",
    "n_vocab = len(set(notes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = './lab8/'\n",
    "with open('./notes_colab', 'rb') as filepath:\n",
    "    notes = pickle.load(filepath)\n",
    "\n",
    "pitchnames = sorted(set(item for item in notes))\n",
    "n_vocab = len(set(notes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    sequence_length = 50\n",
    "\n",
    "    # get all pitch names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "    # print(pitchnames)\n",
    "     # create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "    # print(note_to_int)\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    network_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    # normalize input\n",
    "    network_input = network_input / float(n_vocab)\n",
    "\n",
    "    network_output = np_utils.to_categorical(network_output)\n",
    "\n",
    "    return (network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(network_input, n_vocab):\n",
    "    \"\"\" create the structure of the neural network \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(\n",
    "        512,\n",
    "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "        recurrent_dropout=0.3,\n",
    "        return_sequences=True\n",
    "    ))\n",
    "    model.add(LSTM(512, return_sequences=True, recurrent_dropout=0.3,))\n",
    "    model.add(LSTM(512))\n",
    "    model.add(BatchNorm())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNorm())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, network_input, network_output):\n",
    "    \"\"\" train the neural network \"\"\"\n",
    "    filepath = location + \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath,\n",
    "        monitor='loss',\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        mode='min'\n",
    "    )\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    model.fit(network_input, network_output, epochs=100, batch_size=128, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57127, 50, 1)\n",
      "Epoch 1/100\n",
      "447/447 [==============================] - 463s 1s/step - loss: 5.1635\n",
      "Epoch 2/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 4.6753\n",
      "Epoch 3/100\n",
      "447/447 [==============================] - 461s 1s/step - loss: 4.6206\n",
      "Epoch 4/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 4.6377\n",
      "Epoch 5/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 4.6275\n",
      "Epoch 6/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 4.5754\n",
      "Epoch 7/100\n",
      "447/447 [==============================] - 461s 1s/step - loss: 4.5313\n",
      "Epoch 8/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 4.5099\n",
      "Epoch 9/100\n",
      "447/447 [==============================] - 461s 1s/step - loss: 4.4723\n",
      "Epoch 10/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 4.4297\n",
      "Epoch 11/100\n",
      "447/447 [==============================] - 461s 1s/step - loss: 4.3859\n",
      "Epoch 12/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 4.3501\n",
      "Epoch 13/100\n",
      "447/447 [==============================] - 461s 1s/step - loss: 4.3004\n",
      "Epoch 14/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 4.2514\n",
      "Epoch 15/100\n",
      "447/447 [==============================] - 461s 1s/step - loss: 4.1965\n",
      "Epoch 16/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 4.1388\n",
      "Epoch 17/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 4.0718\n",
      "Epoch 18/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 4.0031\n",
      "Epoch 19/100\n",
      "447/447 [==============================] - 461s 1s/step - loss: 3.9280\n",
      "Epoch 20/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 3.8472\n",
      "Epoch 21/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 3.7675\n",
      "Epoch 22/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 3.6826\n",
      "Epoch 23/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 3.6040\n",
      "Epoch 24/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 3.5213\n",
      "Epoch 25/100\n",
      "447/447 [==============================] - 461s 1s/step - loss: 3.4333\n",
      "Epoch 26/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 3.3522\n",
      "Epoch 27/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 3.2693\n",
      "Epoch 28/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 3.1929\n",
      "Epoch 29/100\n",
      "447/447 [==============================] - 461s 1s/step - loss: 3.1100\n",
      "Epoch 30/100\n",
      "447/447 [==============================] - 461s 1s/step - loss: 3.0371\n",
      "Epoch 31/100\n",
      "447/447 [==============================] - 461s 1s/step - loss: 2.9561\n",
      "Epoch 32/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 2.8866\n",
      "Epoch 33/100\n",
      "447/447 [==============================] - 461s 1s/step - loss: 2.8177\n",
      "Epoch 34/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 2.7416\n",
      "Epoch 35/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 2.6824\n",
      "Epoch 36/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 2.6074\n",
      "Epoch 37/100\n",
      "447/447 [==============================] - 461s 1s/step - loss: 2.5410\n",
      "Epoch 38/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 2.4775\n",
      "Epoch 39/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 2.4040\n",
      "Epoch 40/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 2.3497\n",
      "Epoch 41/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 2.2944\n",
      "Epoch 42/100\n",
      "447/447 [==============================] - 461s 1s/step - loss: 2.2265\n",
      "Epoch 43/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 2.1731\n",
      "Epoch 44/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 2.1119\n",
      "Epoch 45/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 2.0609\n",
      "Epoch 46/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 1.9962\n",
      "Epoch 47/100\n",
      "447/447 [==============================] - 461s 1s/step - loss: 1.9502\n",
      "Epoch 48/100\n",
      "447/447 [==============================] - 461s 1s/step - loss: 1.9006\n",
      "Epoch 49/100\n",
      "447/447 [==============================] - 461s 1s/step - loss: 1.8410\n",
      "Epoch 50/100\n",
      "447/447 [==============================] - 461s 1s/step - loss: 1.7917\n",
      "Epoch 51/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 1.7422\n",
      "Epoch 52/100\n",
      "447/447 [==============================] - 461s 1s/step - loss: 1.7028\n",
      "Epoch 53/100\n",
      "447/447 [==============================] - 461s 1s/step - loss: 1.6576\n",
      "Epoch 54/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 1.5993\n",
      "Epoch 55/100\n",
      "447/447 [==============================] - 461s 1s/step - loss: 1.5733\n",
      "Epoch 56/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 1.5188\n",
      "Epoch 57/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 1.4794\n",
      "Epoch 58/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 1.4384\n",
      "Epoch 59/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 1.3969\n",
      "Epoch 60/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 1.3555\n",
      "Epoch 61/100\n",
      "447/447 [==============================] - 463s 1s/step - loss: 1.3165\n",
      "Epoch 62/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 1.2779\n",
      "Epoch 63/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 1.2439\n",
      "Epoch 64/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 1.2038\n",
      "Epoch 65/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 1.1715\n",
      "Epoch 66/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 1.1354\n",
      "Epoch 67/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 1.1074\n",
      "Epoch 68/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 1.0725\n",
      "Epoch 69/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 1.0375\n",
      "Epoch 70/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 1.0108\n",
      "Epoch 71/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.9780\n",
      "Epoch 72/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.9459\n",
      "Epoch 73/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.9305\n",
      "Epoch 74/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.9009\n",
      "Epoch 75/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.8668\n",
      "Epoch 76/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.8463\n",
      "Epoch 77/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.8219\n",
      "Epoch 78/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.7885\n",
      "Epoch 79/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.7767\n",
      "Epoch 80/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.7538\n",
      "Epoch 81/100\n",
      "447/447 [==============================] - 465s 1s/step - loss: 0.7369\n",
      "Epoch 82/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.7206\n",
      "Epoch 83/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.6926\n",
      "Epoch 84/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.6783\n",
      "Epoch 85/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.6613\n",
      "Epoch 86/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.6513\n",
      "Epoch 87/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.6387\n",
      "Epoch 88/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.6264\n",
      "Epoch 89/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.6109\n",
      "Epoch 90/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.5951\n",
      "Epoch 91/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.5878\n",
      "Epoch 92/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.5767\n",
      "Epoch 93/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.5621\n",
      "Epoch 94/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.5488\n",
      "Epoch 95/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.5443\n",
      "Epoch 96/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.5222\n",
      "Epoch 97/100\n",
      "447/447 [==============================] - 463s 1s/step - loss: 0.5227\n",
      "Epoch 98/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.5147\n",
      "Epoch 99/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.5034\n",
      "Epoch 100/100\n",
      "447/447 [==============================] - 462s 1s/step - loss: 0.4912\n"
     ]
    }
   ],
   "source": [
    "network_input, network_output = prepare_sequences(notes, n_vocab)\n",
    "print(network_input.shape)\n",
    "model = create_network(network_input, n_vocab)\n",
    "train(model, network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./DOS_notes', 'rb') as filepath:\n",
    "    DOS_notes = pickle.load(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_DOS_sequences(DOS_notes, pitchnames, n_vocab):\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "    sequence_length = 50\n",
    "    network_input = []\n",
    "    sequence_in = DOS_notes[0:sequence_length]\n",
    "    # print('sequence_in')\n",
    "    # print(sequence_in)\n",
    "    network_input.append([note_to_int[char] for char in sequence_in])\n",
    "    # print(network_input)\n",
    "    n_patterns = len(network_input)\n",
    "    normalized_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    normalized_input = normalized_input / float(n_vocab)\n",
    "    return (network_input, normalized_input)\n",
    "\n",
    "def load_network(network_input, n_vocab):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(\n",
    "        512,\n",
    "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "        recurrent_dropout=0.3,\n",
    "        return_sequences=True\n",
    "    ))\n",
    "    model.add(LSTM(512, return_sequences=True, recurrent_dropout=0.3,))\n",
    "    model.add(LSTM(512))\n",
    "    model.add(BatchNorm())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNorm())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    model.load_weights(location + 'weights-improvement-100-0.4912-bigger.hdf5')\n",
    "    return model\n",
    "\n",
    "def generate_notes(model, network_input, pitchnames, n_vocab):\n",
    "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
    "    start = 0\n",
    "\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "    \n",
    "    pattern = network_input[start]\n",
    "    prediction_output = []\n",
    "    \n",
    "    # generate 500 notes\n",
    "    for note_index in range(500):\n",
    "        if note_index == 0:\n",
    "            for i in range(50):\n",
    "                prediction_output.append(int_to_note[network_input[0][i]])\n",
    "        prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "        prediction_input = prediction_input / float(n_vocab)\n",
    "\n",
    "        prediction = model.predict(prediction_input, verbose=0)\n",
    "\n",
    "        index = numpy.argmax(prediction)\n",
    "        result = int_to_note[index]\n",
    "        prediction_output.append(result)\n",
    "\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "\n",
    "    return prediction_output\n",
    "    \n",
    "def create_midi(prediction_output):\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
    "        from the notes \"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "\n",
    "    midi_stream.write('midi', fp=location + 'test_output.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network_input, normalized_input = prepare_DOS_sequences(DOS_notes, pitchnames, n_vocab)\n",
    "model = load_network(normalized_input, n_vocab)\n",
    "prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)\n",
    "concat_with_input = []\n",
    "\n",
    "create_midi(prediction_output)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment08.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
