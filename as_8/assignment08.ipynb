{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWo4xJnlTUiy"
   },
   "source": [
    "M2780.002400 Machine Listening (Fall 2020)\n",
    "\n",
    "Instructor: Kyogu Lee (kglee@snu.ac.kr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jB94TKZiTbPX"
   },
   "source": [
    "# Assignment 8: Advanced Neural Networks\n",
    "\n",
    "(40 points)\n",
    "\n",
    "Due Date : This assignment is due by 12:59PM, November 24 (Tuesday)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Assignment\n",
    "\n",
    "This assignment is composed of only one part: Lab assignment, you will need to write Notebook scripts and/or functions as required and submit them electronically (via **ETL**) by the end of the due date. Before the submission, please make sure that the file name is in form of **아무개_hw8.ipynb**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import glob\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, pooling\n",
    "from keras.optimizers import Adam\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsjE1gXxXYO6"
   },
   "source": [
    "# Lab (40pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvvsRSFJXiH2"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "1.   Speech MNIST using CNN (20 pts)\n",
    "\n",
    "  a) Record your speech saying zero, one, two, ..., nine. (You don't need to submit these recordings) Refer to the lab8 materials to create and learn a neural network that classifies spoken digit dataset. Use the trained model to classify your recordings and report the results. \n",
    "\n",
    "  b) Improve the network performance using the techniques introduced in lab8. You can use more or different types of layers, regularization. Data augmentation methods would be also helpful. Compare the results with a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "a7x19YyHXhRd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with my recording: 0.699999988079071\n"
     ]
    }
   ],
   "source": [
    "# a)\n",
    "\n",
    "model = keras.models.load_model(\"mnist_trained\")\n",
    "\n",
    "Y_my_recording = np.eye(10)  # right answer of my recording\n",
    "\n",
    "# preprocess recorded data and stack to make X_my_recording\n",
    "X = []\n",
    "for i in range(10):\n",
    "    wave = np.load('./my_recording/my_{}.npy'.format(i))\n",
    "    mel = librosa.feature.melspectrogram(wave, sr=8000, n_mels=80)\n",
    "    X.append(mel)\n",
    "data_length = []\n",
    "for item in X:\n",
    "    data_length.append(np.shape(item)[1])\n",
    "def pad(mel, max_length):\n",
    "    if np.shape(mel)[1] > max_length:\n",
    "        return mel[:, :max_length]\n",
    "    else:\n",
    "        return np.concatenate((mel, np.zeros((80, max_length-np.shape(mel)[1]))), axis=1)\n",
    "max_length = 36\n",
    "X_pad = []\n",
    "for item in X:\n",
    "    X_pad.append(pad(item, max_length))\n",
    "X_pad = np.asarray(X_pad)\n",
    "X_pad = np.expand_dims(X_pad, axis=3)\n",
    "X_my_recording = X_pad\n",
    "\n",
    "\n",
    "score = model.evaluate(X_my_recording, Y_my_recording, verbose=0)\n",
    "print(\"Accuracy with my recording:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) \n",
    "\n",
    "# prepare train data with data augmentation\n",
    "\n",
    "import glob\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "speech_mnist_data = []\n",
    "speech_mnist_target = []\n",
    "\n",
    "speech_mnist_aug_data = []\n",
    "speech_mnist_aug_target = []\n",
    "\n",
    "speech_mnist = glob.glob('./lab8/speech_mnist/*.npy')\n",
    "speech_mnist_spec_aug = glob.glob('./lab8/speech_mnist/*.npy')\n",
    "for item in speech_mnist:\n",
    "  target, speaker, index = item.split('/')[-1].split('.npy')[0].split('_')\n",
    "  wavs = np.load(item)\n",
    "  mel = librosa.feature.melspectrogram(wavs, sr=8000, n_mels=80)\n",
    "  speech_mnist_data.append(mel)\n",
    "  speech_mnist_target.append(np.eye(10)[(int)(target)])\n",
    "\n",
    "for item in speech_mnist_spec_aug:\n",
    "    target, speaker, index = item.split('/')[-1].split('.npy')[0].split('_')\n",
    "    wavs = np.load(item)\n",
    "    mel = librosa.feature.melspectrogram(wavs, sr=8000, n_mels = 80)\n",
    "    f_frame = np.random.randint(0, 20)\n",
    "    t_frame = np.random.randint(0, 3)\n",
    "    f_loc = np.random.randint(0, mel.shape[0] - f_frame)\n",
    "    t_loc = np.random.randint(0, mel.shape[1] - t_frame)\n",
    "    for i in range(0, f_frame):\n",
    "        mel[f_loc + i, :] = 0\n",
    "    for i in range(0, t_frame):\n",
    "        mel[:, t_loc + t_frame] = 0\n",
    "    speech_mnist_aug_data.append(mel)\n",
    "    speech_mnist_aug_target.append(np.eye(10)[(int)(target)])\n",
    "\n",
    "data_length = []\n",
    "for item in speech_mnist_data:\n",
    "  data_length.append(np.shape(item)[1])\n",
    "\n",
    "def pad(mel, max_length):\n",
    "  if np.shape(mel)[1] > max_length:\n",
    "    return mel[:,:max_length]\n",
    "  else:\n",
    "    return np.concatenate((mel, np.zeros((80, max_length-np.shape(mel)[1]))), axis=1)\n",
    "\n",
    "max_length = np.max(data_length)\n",
    "speech_mnist_data_pad = []\n",
    "speech_mnist_aug_data_pad = []\n",
    "for item in speech_mnist_data:\n",
    "  speech_mnist_data_pad.append(pad(item, max_length))\n",
    "for item in speech_mnist_aug_data:\n",
    "    speech_mnist_aug_data_pad.append(pad(item, max_length))\n",
    "\n",
    "speech_mnist_data_pad = np.asarray(speech_mnist_data_pad)\n",
    "speech_mnist_data_pad = np.expand_dims(speech_mnist_data_pad, axis=3)\n",
    "speech_mnist_aug_data_pad = np.asarray(speech_mnist_aug_data_pad)\n",
    "speech_mnist_aug_data_pad = np.expand_dims(speech_mnist_aug_data_pad, axis=3)\n",
    "speech_mnist_data_all_pad = np.concatenate((speech_mnist_data_pad, speech_mnist_aug_data_pad), axis=0)\n",
    "speech_mnist_target_all = np.concatenate((speech_mnist_target, speech_mnist_aug_target), axis=0)\n",
    "speech_mnist_target = np.asarray(speech_mnist_target)\n",
    "\n",
    "X_train, _, Y_train, _ = train_test_split(speech_mnist_data_all_pad, speech_mnist_target_all, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new model(decreased the dropout rate)\n",
    "\n",
    "new_model = Sequential()\n",
    "new_model.add(Conv2D(32,(3,3),activation='relu', input_shape=(80,36,1)))\n",
    "new_model.add(Conv2D(32,(3,3),activation='relu'))\n",
    "new_model.add(pooling.MaxPooling2D(pool_size=(2,2)))\n",
    "new_model.add(Dropout(0.2))\n",
    "new_model.add(Conv2D(32,(3,3),activation='relu'))\n",
    "new_model.add(Conv2D(32,(3,3),activation='relu'))\n",
    "new_model.add(pooling.MaxPooling2D(pool_size=(2,2)))\n",
    "new_model.add(Dropout(0.2))\n",
    "new_model.add(Conv2D(32,(3,3),activation='relu'))\n",
    "new_model.add(Conv2D(32,(3,3),activation='relu'))\n",
    "new_model.add(pooling.MaxPooling2D(pool_size=(2,2)))\n",
    "new_model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "new_model.add(Flatten())\n",
    "new_model.add(Dense(128, activation='relu'))\n",
    "new_model.add(Dropout(0.5))\n",
    "new_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "opt = Adam(lr=0.001)\n",
    "new_model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3196, 80, 36, 1)\n",
      "(3196, 10)\n",
      "Epoch 1/50\n",
      "45/45 [==============================] - ETA: 0s - loss: 2.2481 - accuracy: 0.1380WARNING:tensorflow:8 out of the last 24 calls to <function Model.make_test_function.<locals>.test_function at 0x7fceda784c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 2.2481 - accuracy: 0.1380 - val_loss: 2.0885 - val_accuracy: 0.2500\n",
      "Epoch 2/50\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 2.0933 - accuracy: 0.2330 - val_loss: 1.8104 - val_accuracy: 0.4094\n",
      "Epoch 3/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 1.9932 - accuracy: 0.2987 - val_loss: 1.7061 - val_accuracy: 0.4344\n",
      "Epoch 4/50\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 1.8265 - accuracy: 0.3428 - val_loss: 1.4124 - val_accuracy: 0.4844\n",
      "Epoch 5/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 1.5602 - accuracy: 0.4350 - val_loss: 1.1692 - val_accuracy: 0.6031\n",
      "Epoch 6/50\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 1.3510 - accuracy: 0.5170 - val_loss: 1.0398 - val_accuracy: 0.6094\n",
      "Epoch 7/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 1.2948 - accuracy: 0.5348 - val_loss: 0.9404 - val_accuracy: 0.6219\n",
      "Epoch 8/50\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 1.1663 - accuracy: 0.5751 - val_loss: 0.8738 - val_accuracy: 0.6625\n",
      "Epoch 9/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 1.1116 - accuracy: 0.6095 - val_loss: 0.9149 - val_accuracy: 0.6531\n",
      "Epoch 10/50\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 1.0463 - accuracy: 0.6147 - val_loss: 0.7676 - val_accuracy: 0.6781\n",
      "Epoch 11/50\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 0.9721 - accuracy: 0.6481 - val_loss: 0.7044 - val_accuracy: 0.7031\n",
      "Epoch 12/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.9021 - accuracy: 0.6686 - val_loss: 0.6673 - val_accuracy: 0.7469\n",
      "Epoch 13/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.8527 - accuracy: 0.6836 - val_loss: 0.6018 - val_accuracy: 0.7625\n",
      "Epoch 14/50\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 0.8360 - accuracy: 0.6999 - val_loss: 0.6452 - val_accuracy: 0.7469\n",
      "Epoch 15/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.7989 - accuracy: 0.7076 - val_loss: 0.5763 - val_accuracy: 0.7750\n",
      "Epoch 16/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.7430 - accuracy: 0.7333 - val_loss: 0.5418 - val_accuracy: 0.7906\n",
      "Epoch 17/50\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 0.7221 - accuracy: 0.7403 - val_loss: 0.5663 - val_accuracy: 0.7937\n",
      "Epoch 18/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.6988 - accuracy: 0.7524 - val_loss: 0.4808 - val_accuracy: 0.8375\n",
      "Epoch 19/50\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 0.6453 - accuracy: 0.7604 - val_loss: 0.4774 - val_accuracy: 0.8156\n",
      "Epoch 20/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.6081 - accuracy: 0.7782 - val_loss: 0.4472 - val_accuracy: 0.8250\n",
      "Epoch 21/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.5941 - accuracy: 0.7837 - val_loss: 0.4304 - val_accuracy: 0.8406\n",
      "Epoch 22/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.5967 - accuracy: 0.7876 - val_loss: 0.4169 - val_accuracy: 0.8469\n",
      "Epoch 23/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.5868 - accuracy: 0.7973 - val_loss: 0.4052 - val_accuracy: 0.8469\n",
      "Epoch 24/50\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 0.5875 - accuracy: 0.7949 - val_loss: 0.3909 - val_accuracy: 0.8656\n",
      "Epoch 25/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.5496 - accuracy: 0.8001 - val_loss: 0.4182 - val_accuracy: 0.8500\n",
      "Epoch 26/50\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 0.5766 - accuracy: 0.8001 - val_loss: 0.4646 - val_accuracy: 0.8406\n",
      "Epoch 27/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.5645 - accuracy: 0.8077 - val_loss: 0.3542 - val_accuracy: 0.8844\n",
      "Epoch 28/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.5177 - accuracy: 0.8241 - val_loss: 0.3458 - val_accuracy: 0.8781\n",
      "Epoch 29/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.4813 - accuracy: 0.8289 - val_loss: 0.3214 - val_accuracy: 0.8969\n",
      "Epoch 30/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.4713 - accuracy: 0.8359 - val_loss: 0.3076 - val_accuracy: 0.8906\n",
      "Epoch 31/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.4457 - accuracy: 0.8404 - val_loss: 0.3379 - val_accuracy: 0.8781\n",
      "Epoch 32/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.4496 - accuracy: 0.8387 - val_loss: 0.3035 - val_accuracy: 0.9000\n",
      "Epoch 33/50\n",
      "45/45 [==============================] - 4s 99ms/step - loss: 0.4250 - accuracy: 0.8380 - val_loss: 0.3043 - val_accuracy: 0.8844\n",
      "Epoch 34/50\n",
      "45/45 [==============================] - 4s 99ms/step - loss: 0.4176 - accuracy: 0.8456 - val_loss: 0.2739 - val_accuracy: 0.9031\n",
      "Epoch 35/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.4033 - accuracy: 0.8585 - val_loss: 0.3039 - val_accuracy: 0.9094\n",
      "Epoch 36/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.4200 - accuracy: 0.8515 - val_loss: 0.3214 - val_accuracy: 0.8906\n",
      "Epoch 37/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.4339 - accuracy: 0.8387 - val_loss: 0.2720 - val_accuracy: 0.8906\n",
      "Epoch 38/50\n",
      "45/45 [==============================] - 4s 100ms/step - loss: 0.4801 - accuracy: 0.8366 - val_loss: 0.3938 - val_accuracy: 0.8687\n",
      "Epoch 39/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.4115 - accuracy: 0.8574 - val_loss: 0.2763 - val_accuracy: 0.9031\n",
      "Epoch 40/50\n",
      "45/45 [==============================] - 5s 101ms/step - loss: 0.3924 - accuracy: 0.8644 - val_loss: 0.2317 - val_accuracy: 0.9125\n",
      "Epoch 41/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.3614 - accuracy: 0.8762 - val_loss: 0.2455 - val_accuracy: 0.9219\n",
      "Epoch 42/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.3518 - accuracy: 0.8769 - val_loss: 0.2431 - val_accuracy: 0.9062\n",
      "Epoch 43/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.3485 - accuracy: 0.8783 - val_loss: 0.2254 - val_accuracy: 0.9219\n",
      "Epoch 44/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.3372 - accuracy: 0.8853 - val_loss: 0.2216 - val_accuracy: 0.9219\n",
      "Epoch 45/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.3287 - accuracy: 0.8846 - val_loss: 0.2117 - val_accuracy: 0.9250\n",
      "Epoch 46/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.3321 - accuracy: 0.8856 - val_loss: 0.2149 - val_accuracy: 0.9250\n",
      "Epoch 47/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.3555 - accuracy: 0.8842 - val_loss: 0.2030 - val_accuracy: 0.9344\n",
      "Epoch 48/50\n",
      "45/45 [==============================] - 4s 97ms/step - loss: 0.3148 - accuracy: 0.8870 - val_loss: 0.2031 - val_accuracy: 0.9438\n",
      "Epoch 49/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.3163 - accuracy: 0.8849 - val_loss: 0.1868 - val_accuracy: 0.9438\n",
      "Epoch 50/50\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 0.3065 - accuracy: 0.8877 - val_loss: 0.1908 - val_accuracy: 0.9375\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "hist = new_model.fit(X_train, Y_train, batch_size=64, epochs=50, validation_split = 0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with my recording: 1.0\n"
     ]
    }
   ],
   "source": [
    "score = new_model.evaluate(X_my_recording, Y_my_recording, verbose=0)\n",
    "print(\"Accuracy with my recording:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilgkgKdxdRIB"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "2.   Music Composer using LSTM (20 pts)\n",
    "\n",
    "\n",
    "Refer to the lab8 materials to create and learn a neural network that composes music autoregressively.  Change the input feature and model structure to use 50 notes to generate the next note. Use the trained model to generate 500 notes of music starting from the first 50 notes of DOS.mid on the dataset. Save and submit the generated music as midi file. Your submission should include the trained model weights as hdf5 file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRoTzo28Xb9J"
   },
   "outputs": [],
   "source": [
    "# !!!! your code here !!!!\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment08.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
