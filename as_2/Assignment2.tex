\documentclass[12pt]{article}%
\usepackage{amsfonts}
\usepackage{fancyhdr}
\usepackage{comment}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2.2cm, right=2.2cm]%
{geometry}
\usepackage{times}
\usepackage{amsmath}
\usepackage{changepage}
\usepackage{amssymb}
\usepackage{graphicx}%
\setcounter{MaxMatrixCols}{30}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\textbf{#1.} }{\ \rule{0.5em}{0.5em}}

\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}

\begin{document}

\title{Assignment 2}
\author{Changwoon Choi \\ 2020-20206}
\date{\today}
\maketitle

\section{Linear predictive coding}
Consider a signal $x(n)$ given by \\
\begin{center}
	$x(n) = [1, -1, 0, 1, 0, -1, 1, 1]$
\end{center}
Given two linear predictive coefficients with order 3 and 5 specified by $A_1 = [-0.17, -0.5, 0.17]$ and $A_2 = [-0.21, -0.38, 0.21, 0.25, 0.01]$, respectively,
\subsection{Find $\hat{x_1}(n)$ and $\hat{x_2}(n)$)}
$\hat{x_1}(n) = \sum_{k=1}^{3} a_k x(n-k) = [0, -0.17, -0.33, 0.67, -0.34, -0.5, 0.34, 0.33]$ \\ \\
$\hat{x_2}(n) = \sum_{k=1}^{5} a_k x(n-k) = [0, -0.21, -0.17, 0.59, -0.17, -0.62, 0.41, 0.42]$

\subsection{Compute the residual errors $e_1 (n)$ and $e_2 (n)$.}
$e_1 (n) = x(n) - \hat{x_1}(n) = [1, -0.83, 0.33, 0.33, 0.34, -0.5, 0.66, 0.67]$ \\ \\
$e_2 (n) = x(n) - \hat{x_2}(n) = [1, -0.79, 0.17, 0.41, 0.17, -0.38, 0.59, 0.58]$
\subsection{Compute the mean square errors (MSE). The MSE is given by $MSE = {1 \over N} \sum_{n=0}^{N-1}[e(n)]^2$ Are the results consistent with your expectation?}
$MSE_1 = {1 \over N} \sum_{n=0}^{N-1}[e_1 (n)]^2 = 0.3946$ \\ \\
$MSE_2 = {1 \over N} \sum_{n=0}^{N-1}[e_2 (n)]^2 = 0.3348625$ \\ \\
$\hat{x_2}(n)$ was expected to have better(lower) MSE. Since the higher the filter order, the closer the approximation is to the signal's spectrum coefficients. Thus, the results were consistent with my initial expectation.

\end{document}