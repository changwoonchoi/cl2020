{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oR3pclAb4Ht8"
   },
   "source": [
    "M2780.002400 Machine Listening (Fall 2020)\n",
    "\n",
    "Instructor: Kyogu Lee (kglee@snu.ac.kr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbvqAdck4JHM"
   },
   "source": [
    "# Assignment 6: Neural Networks\n",
    "\n",
    "(100 points)\n",
    "\n",
    "**Due Date : This assignment is due by 12:59PM, October 20 (Tuesday)**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Assignment\n",
    "\n",
    "Each assignment is composed of two parts: a) in the theory part, you are required to solve the problem set, write down your answers on paper (also, typing your answers using PC or tablet PC is fine), and upload the scanned version (**아무개_hw6.pdf**) via **ETL**; b) for the lab assignment, you will need to write Notebook scripts (**아무개_hw6.ipynb**) and/or functions as required and submit them electronically (via **ETL**) by the end of the due date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rSnOmJBntsv"
   },
   "source": [
    "# Theory (50 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhUSsvScn2uq"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "1.   Model Representation (10 pts)\n",
    "\n",
    "\n",
    " Suppose you want to train the digit classifier using a simple neural network (i.e. the number of output nodes is 10). The input is a square gray image with 100 pixels. (i.e. (10x10) for one image). If your network has only one hidden layer with 50 nodes, how many parameters will be updated in the training phase (HINT: do not forget the bias node)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbpmsglQuuU0"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "2.   Feedforward Propagation (10 pts)\n",
    "\n",
    "\n",
    " You are using the neural network pictured below and have learned the parameters $\\Theta^{(1)} = \\begin{bmatrix}\n",
    "    1 & 2.1 & 1.3 \\\\\n",
    "    1 & 0.6 & -1.2 \\\\\n",
    "\\end{bmatrix}$ (used to compute $a^{(3)}$) and $\\Theta^{(2)} = \\begin{bmatrix}\n",
    "    1 & 4.5 & 3.1 \\\\\n",
    "\\end{bmatrix}$  (used to compute $a^{(3)}$, as function of $a^{(2)}$). Suppose you swap the parameters for the first hidden layer between its two units. So  $\\Theta^{(1)} = \\begin{bmatrix}\n",
    "    1 & 0.6 & -1.2 \\\\\n",
    "    1 & 2.1 & 1.3 \\\\\n",
    "\\end{bmatrix}$and $\\Theta^{(2)} = \\begin{bmatrix}\n",
    "    1 & 4.5 & 3.1 \\\\\n",
    "\\end{bmatrix}$. How will this change the value of the output $h_{\\Theta}(x)$ ? \n",
    "\n",
    "\n",
    " \n",
    "![](https://gdurl.com/dAiN)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*   a. It will decrease\n",
    "*   b. It will increase\n",
    "*   c. It will stay same\n",
    "*   d. Insufficient information to tell: it may increase or decrease\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8PrJQMTwTXJ"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "3.   Back Propagation (10 pts)\n",
    "\n",
    "\n",
    " When training the neural networks, what are the typical steps for using a gradient descent algorithm?\n",
    "\n",
    "1) Compute the error between the actual value and the predicted value\n",
    "\n",
    "2) Reiterate until you find the best weights of network (until the error is sufficiently small)\n",
    "\n",
    "3) Pass an input through the network and get values from output layer\n",
    "\n",
    "4) Initialize random weight and bias\n",
    "\n",
    "5) Go to each neuron which contributes to the error and change its respective values to reduce the error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*   a. 1 ➜ 2 ➜ 3 ➜ 4 ➜ 5\n",
    "*   b. 5 ➜ 4 ➜ 3 ➜ 2 ➜ 1\n",
    "*   c. 3 ➜ 2 ➜ 1 ➜ 5 ➜ 4\n",
    "*   d. 4 ➜ 3 ➜ 1 ➜ 5 ➜ 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5q301bhDwmR1"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "4.   Gradient Computation (20 pts)\n",
    "\n",
    "Given a quadratic cost function\n",
    "\n",
    "\n",
    "$J(\\Theta) = \\frac{1}{2}\\sum_{k \\in K} ((h_{\\Theta}(x))_{k} - y_{k})^{2} $\n",
    "\n",
    "\n",
    "and \"error\" of cost for $a_{j}^{(l)}$ (unit $j$ in layer $l$)\n",
    "\n",
    "\n",
    "$\\delta_{j}^{(l)} = \\frac{\\partial }{\\partial z_{j}^{(l)}} J(\\Theta)$\n",
    "\n",
    "\n",
    "drive the following equations:\n",
    "\n",
    "\n",
    "\\\\\n",
    "\n",
    "\n",
    "\n",
    "a) $\\delta^{(L)} = (a^{(L)} - y).*g^{'}(z^{(L)})$\n",
    "\n",
    "b) $\\delta^{(l)} = (\\Theta^{(l)})^{T} \\delta^{(l+1)}.*g^{'}(z^{(l)})$\n",
    "\n",
    "c) $\\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = a_{j}^{(l)} \\delta_{i}^{l+1}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZKItpKtkuh1"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "5.   (optional) Gradient Computation (20 pts)\n",
    "\n",
    " Repeat Problem 4 for the cross-entropy cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulLAwXQUk2T0"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "6.   (optional) Convexity of Cost Functions (20 pts)\n",
    "\n",
    "\n",
    "Assume we use a sigmoid function for output layer activation in Neural Network; i.e. $a_L = g_L (z_L) = \\frac{1}{1 + e^{-zL}}$, Prove that the cross-entropy loss function is convex (hint: for a convex function $f(x),f^{''}(x)=\\frac{d^2f}{dx^2} \\geq 0, \\forall x \\in \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Rs_XOPK4Thp"
   },
   "source": [
    "# Lab (50pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTOGA3u44W9b"
   },
   "source": [
    "For all lab assignments, submit your Notebook file (**아무개_hw6.ipynb**) via **ETL**. The Notebook file should be named with your full name and the homework number – e.g., **아무개_hw6.ipynb**. \n",
    "\n",
    "In addition to writing Python scripts and/or functions in your Notebook file, there are also questions you’ll have to answer. For such questions, you should provide answers in your Notebook file using Text sections.\n",
    "\n",
    "**Please submit the codes executed (make sure that the results of your codes are visible in the submitted assignments) so that the grader can check whether the code is working or not.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OajMlkaBl0HO"
   },
   "source": [
    "\n",
    "1.   binary MNIST from Scratch (30 pts)\n",
    "\n",
    "Choose two digits from the MNIST handwriting data. Make a subset of the MNIST dataset containing only the chosen two digits. Separate the dataset to training and testing data. The ratio of the training and testing data should be 6 to 1. Make and train a neural network from scratch. Use the code provided by lab6 and expand the network to have three hidden layers. The number of input nodes should be 784 (28X28) and the number of output nodes should be 2 (0, 1 binary one hot vector). The number of nodes for the hidden layers should be scalable. The activation function for the last layer (output layer) should be sigmoid and the activation for the rest of the network should be relu. Plot the cost and the confusion matrix of your trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "AMKmy6Yp78YX"
   },
   "outputs": [],
   "source": [
    "# Download Mnist\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose class '0', and '1'\n",
    "\n",
    "X_train_subset = X_train[np.logical_or(Y_train == 0, Y_train == 1),:,:]\n",
    "Y_train_subset = Y_train[np.logical_or(Y_train == 0, Y_train == 1)]\n",
    "\n",
    "X_test_subset = X_test[np.logical_or(Y_test == 0, Y_test == 1),:,:]\n",
    "Y_test_subset = Y_test[np.logical_or(Y_test == 0, Y_test == 1)]\n",
    "\n",
    "X_train_subset = X_train_subset.reshape(-1, 784)\n",
    "X_test_subset = X_test_subset.reshape(-1, 784)\n",
    "X_train_subset = X_train_subset.astype('float32')\n",
    "X_test_subset = X_test_subset.astype('float32')\n",
    "X_train_subset /= 255\n",
    "X_test_subset /= 255\n",
    "\n",
    "y_train_all = []\n",
    "for item in Y_train_subset:\n",
    "    y_train_all.append(np.eye(2)[item])\n",
    "y_train_all = np.asarray(y_train_all)\n",
    "\n",
    "y_test_all = []\n",
    "for item in Y_test_subset:\n",
    "    y_test_all.append(np.eye(2)[item])\n",
    "y_test_all = np.asarray(y_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12665, 2)\n",
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_train_all.shape)\n",
    "print(y_train_all[23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = y_train_all.shape[0] # training set size\n",
    "nn_input_dim = 784 # input layer dimensionality\n",
    "nn_output_dim = 2 # output layer dimensionality\n",
    "\n",
    "# Gradient descent parameters (I picked these by hand)\n",
    "epsilon = 0.01 # learning rate for gradient descent\n",
    "reg_lambda = 0.01 # regularization strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "\n",
    "def calculate_loss(model):\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'], model['b3']\n",
    "    # Forward propagation to calculate our predictions\n",
    "    z1 = X_train_subset.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    a2 = np.tanh(z2)\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    exp_scores = np.exp(z3)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    # Calculating the loss\n",
    "    corect_logprobs = -np.log([probs[i,np.nonzero(y_train_all)[(1)][i].astype('int64')] for i in range(num_examples)])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "    # Add regulatization term to loss (optional)\n",
    "    data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "    return 1./num_examples * data_loss\n",
    "\n",
    "def predict(model, x):\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'], model['b3']\n",
    "    # Forward propagation\n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    a2 = np.tanh(z2)\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    exp_scores = np.exp(z3)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return np.argmax(probs, axis=1)\n",
    "\n",
    "def calculate_accuracy(model):\n",
    "    predicted = predict(model, X_test_subset)\n",
    "    correct = 0\n",
    "    for i in range(y_test_all.shape[0]):\n",
    "        if y_test_all[i][predicted[i]] != 0:\n",
    "            correct += 1 \n",
    "    print('Test accuracy : {}%'.format(100.*correct/y_test_all.shape[0]))\n",
    "    \n",
    "def build_model(nn_hdim_1, nn_hdim_2, num_passes=20000, print_loss=False):\n",
    "    \n",
    "    # Initialize the parameters to random values. We need to learn these.\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(nn_input_dim, nn_hdim_1) / np.sqrt(nn_input_dim)\n",
    "    b1 = np.zeros((1, nn_hdim_1))\n",
    "    W2 = np.random.randn(nn_hdim_1, nn_hdim_2) / np.sqrt(nn_hdim_1)\n",
    "    b2 = np.zeros((1, nn_hdim_2))\n",
    "    W3 = np.random.randn(nn_hdim_2, nn_output_dim) / np.sqrt(nn_hdim_2)\n",
    "    b3 = np.zeros((1, nn_output_dim))\n",
    "\n",
    "    # This is what we return at the end\n",
    "    model = {}\n",
    "    \n",
    "    # Gradient descent. For each batch...\n",
    "    for i in range(0, num_passes):\n",
    "\n",
    "        # Forward propagation\n",
    "        z1 = X_train_subset.dot(W1) + b1\n",
    "        a1 = np.tanh(z1)\n",
    "        z2 = a1.dot(W2) + b2\n",
    "        a2 = np.tanh(z2)\n",
    "        z3 = a2.dot(W3) + b3\n",
    "        exp_scores = np.exp(z3)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "        # Backpropagation\n",
    "        '''\n",
    "        delta3 = (probs - train_labels) / data_size\n",
    "        dW2 = (a1.T).dot(delta3)\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n",
    "        dW1 = np.dot(train_dataset.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)\n",
    "        '''\n",
    "        delta4 = (probs - y_train_all) / y_train_all.shape[0]\n",
    "        dW3 = (a2.T).dot(delta4)\n",
    "        db3 = np.sum(delta4, axis=0, keepdims=True)\n",
    "        delta3 = delta4.dot(W3.T) * (1 - np.power(a2, 2))\n",
    "        dW2 = np.dot(z1.T, delta3)\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n",
    "        dW1 = np.dot(X_train_subset.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "        # Add regularization terms (b1 and b2 don't have regularization terms)\n",
    "        dW3 += reg_lambda * W3\n",
    "        dW2 += reg_lambda * W2\n",
    "        dW1 += reg_lambda * W1\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        W1 += -epsilon * dW1\n",
    "        b1 += -epsilon * db1\n",
    "        W2 += -epsilon * dW2\n",
    "        b2 += -epsilon * db2\n",
    "        W3 += -epsilon * dW3\n",
    "        b3 += -epsilon * db3\n",
    "        \n",
    "        # Assign new parameters to the model\n",
    "        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3': W3, 'b3': b3}\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            loss.append(calculate_loss(model))\n",
    "        \n",
    "        if print_loss and i == 0:\n",
    "            print(\"Loss after iteration %i: %f\" %(i, calculate_loss(model)))\n",
    "        if print_loss and i % 1000 == 999:\n",
    "            print(\"Loss after iteration %i: %f\" %(i, calculate_loss(model)))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.560644\n"
     ]
    }
   ],
   "source": [
    "model = build_model(128, 128, num_passes=3000, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFG4e0o_77OW"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "2.   binary MNIST using keras (10 pts)\n",
    "\n",
    "Use the dataset created at lab problem 1. Make and train a neural network model resembling the neural network model used at lab problem 1 using keras. Verify your network by comparing the accuracy with the result from lab problem 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLdyTVZc8B8h"
   },
   "outputs": [],
   "source": [
    "# !!!! your code here !!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iS-Rnmhz4kka"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "3.   MNIST using keras  (10 pts)\n",
    "\n",
    "\n",
    "Refer to the lab6 materials to create and learn a neural network that classifies MNIST handwriting data and organize the results using keras. Change the number of layers, number of neurons, and activation functions and experiment with three different settings and verify the accuracy of over the test data. Draw a confusion matrix of the most accurate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehO71PKoq0YP"
   },
   "outputs": [],
   "source": [
    "# !!!! your code here !!!!\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment06.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
